## 一致性哈希

### 常用场景

常用于负载均衡，数据分片等场景。

### 原理

与最简单的负载均衡算法类似，一致性哈希也是基于取模运算。

主要结构是一个哈希环，上面有 **2^32** 个槽位，从 **0 到 2^32-1**，顺时针旋转，0 与 2^32-1处汇合。如果用服务器节点作为槽位，那么可以用 IP 地址作为标志，**hash(NODE_IP) % 2^32** 的结果可以作为节点所处的槽位。

![CS-1](https://github.com/m4n5ter/m4n5ter.github.io/blob/main/assets/consistent_hashing_1.png?raw=true)

当有多个节点加入到哈希环中，看起来会像是这样。

![CH2](https://github.com/m4n5ter/m4n5ter.github.io/blob/main/assets/consistent_hashing_2.png?raw=true)

节点会分布在哈希环的各处，这里我们可以将那些被一致性哈希算法分配（可以看作被负载均衡器分配的任务）的东西称作任务，将任务的特征对**2^32**取模，我们可以得到一个槽位，这个操作可以看成 **hash(任务) % 2^32** 。接下来任务当然需要被工作节点接收，寻找工作节点的方式就是**从通过对任务进行hash后取模得到的槽位开始，顺时针遇到的第一个节点会得到这个任务**。

#### 与简单取模的区别

简单取模无法应对工作节点会扩容或者缩容的场景，比如在一个分布式缓存系统中，有 5 个节点，每个缓存节点都存储了大量缓存，我们通过一个 key 找到对应的 value 的方式一般就是对 key 做一次 hash 后再对 5 取模，得到存储这个 key 的节点，并从该节点中得到相应的 value。

如果我们需要增加或减少一个节点，比如节点增加到了6个，那么每个 key 的 hash 都需要对 6 取模，得到的结果（也就是存储着相应 K-V 的节点）会发生变化，也就是无法通过原来的 key 找到原来存储着这对 K-V 的节点了，那么就必须调整大量节点的缓存分布。也就意味着这时大量的缓存会失效，在分布式场景中极易造成雪崩。

而在一致性哈希中，如果新增或减少一个节点，相应的操作就是在哈希环上的某个槽位插入或删除一个节点，就只需要调整对应槽位相邻节点的缓存分布。无非就是在删除一个节点时，将这个节点中存储的 K-V 转移到顺时针下一个节点中；在增加一个节点的时候，在新增节点顺时针第一个节点中，寻找落在新增节点的逆时针方向的 K-V 直到遇到逆时针方向的下一个节点，并将它们调整到新节点中。

![CH3](https://github.com/m4n5ter/m4n5ter.github.io/blob/main/assets/consistent_hashing_3.png?raw=true)

就像图中所示，在新增节点时红色部分会被调整到新节点中，绿色部分保留在原本的节点中。

在有大量节点的分布式环境中，这样调整的代价就会小很多。

### 虚拟节点

在上面的内容中，很容易发现一个新的问题，那就是当节点的数量并不多的时候，很容易存在任务分配不均的情况，可能大量的任务落到的槽位会集中在某块区域，而其它区域的槽位则没有那么多的任务，当一个节点不堪重负被压垮后，那么压力会顺延给顺时针的下一个节点，形成雪崩。这时候就需要引入虚拟节点了。

#### ![CH4](https://github.com/m4n5ter/m4n5ter.github.io/blob/main/assets/consistent_hashing_4.png?raw=true)

比如对节点多次 hash 并取模，得到若干个槽位，这些槽位均代表该节点，那么就可以大大减轻任务分布不均的问题了。
